{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kareef Ullah-rising senior at Wootton High School\n",
    "<br>\n",
    "Worked in the lab from June 17th-August 22nd\n",
    "<br>\n",
    "Focused on creating simulations/other miscellaneous work\n",
    "<br>\n",
    "3 important simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First couple of weeks: \n",
    "<br>\n",
    "Introduced to git/github/python/VScode and graspy package (go through tutorial on https://graspy.neurodata.io/tutorial)\n",
    "<br>\n",
    "Learned process of git add -> git commit -> git push\n",
    "<br>\n",
    "Made small PR to graspy to make tutorial for Adjacency Spectral Embed functional\n",
    "<br>\n",
    "Read papers: Connectal coding, CASC, Two Truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDT simulation\n",
    "<br>\n",
    "Purpose: To demonstrate issues with the latent distribution (nonpar) test when the 2 graphs have different number of vertices\n",
    "<br>\n",
    "Shows type 1 error from the nonpar test on 2 block graphs with varying number of vertices\n",
    "<br>\n",
    "Concluded that type 1 error is more common when:\n",
    "- There is a large difference between the number of vertices of the two graphs \n",
    "<br>\n",
    "- The number of vertices for either of the graphs is low\n",
    "<br>\n",
    "Code for LDT simulation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "\n",
    "from graspy.inference import LatentDistributionTest\n",
    "from graspy.simulations import sbm\n",
    "from graspy.utils import symmetrize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8888)\n",
    "\n",
    "B = [[0.5, 0.2], [0, 0.05]]\n",
    "B = symmetrize(B)\n",
    "k = 2\n",
    "tests = 10\n",
    "start = 50\n",
    "stop = 200\n",
    "diff1 = 50\n",
    "diff2 = 100\n",
    "reps = 10\n",
    "alpha = 0.05\n",
    "ns = []\n",
    "ms = []\n",
    "newms = []\n",
    "error_list = []\n",
    "temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(start, stop, diff1):\n",
    "    ns.append(n)\n",
    "    for m in range(n, n + (diff2 * reps), diff2):\n",
    "        print(f\"Running tests for n={n}, m={m}\")\n",
    "        cn = [n // k] * k\n",
    "        cm = [m // k] * k\n",
    "        A1 = sbm(cn, B)\n",
    "        A2 = sbm(cm, B)\n",
    "        valid = 0\n",
    "        ldt = LatentDistributionTest(n_components=2)\n",
    "        for _ in range(tests):\n",
    "            p = ldt.fit(A1, A2)\n",
    "            if p < 0.05:\n",
    "                valid += 1\n",
    "        error = valid / tests\n",
    "        print(f\"Error was {error}\")\n",
    "        temp.append(error)\n",
    "        ms.append(m - n)\n",
    "    error_list.append(temp)\n",
    "    temp = []\n",
    "\n",
    "for num in ms:\n",
    "    if num not in newms:\n",
    "        newms.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(error_list, index=ns, columns=newms)\n",
    "sns.heatmap(df, annot=True, linewidths=0.5)\n",
    "plt.title(\"Variation of Type 1 Error with Different Cases of LDT\")\n",
    "plt.xlabel(\"m - n\")\n",
    "plt.ylabel(\"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do's and don't experiment 7 simulations\n",
    "<br>\n",
    "Two populations, sample DCSBMs from them\n",
    "<br>\n",
    "For pop1, the promiscuity parameter is 0.5 for all verts\n",
    "<br>\n",
    "For pop2, vertex 1 has a different promiscuity parameter\n",
    "<br>\n",
    "Use omnibus embedding and DCorr test to see whether each vertex is different\n",
    "<br>\n",
    "Purpose: Want to make sure that DCorr test yields p-values consistent to prediction (significant p-value for vertex 1 but insignificant for all others)\n",
    "<br>\n",
    "Simulations to see distribution of p-values for a single node and save p-values for all nodes\n",
    "<br>\n",
    "Code for experiment 7 simulations below (node-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from mgcpy.hypothesis_tests.transforms import k_sample_transform\n",
    "from mgcpy.independence_tests.dcorr import DCorr\n",
    "from tqdm import tqdm\n",
    "from graspy.embed import OmnibusEmbed\n",
    "from graspy.plot import heatmap, pairplot\n",
    "from graspy.simulations import p_from_latent, sample_edges\n",
    "from graspy.utils import cartprod\n",
    "from src.utils import n_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_to_full(block_mat, inverse, shape):\n",
    "    block_map = cartprod(inverse, inverse).T\n",
    "    mat_by_edge = block_mat[block_map[0], block_map[1]]\n",
    "    full_mat = mat_by_edge.reshape(shape)\n",
    "    return full_mat\n",
    "\n",
    "\n",
    "def dcsbm(vertex_assignments, block_p, degree_corrections, return_p_mat=False):\n",
    "    n_verts = len(vertex_assignments)\n",
    "    p_mat = block_to_full(block_p, vertex_assignments, (n_verts, n_verts))\n",
    "    p_mat = p_mat * np.outer(degree_corrections, degree_corrections)\n",
    "    if return_p_mat:\n",
    "        return p_mat\n",
    "    else:\n",
    "        return sample_edges(p_mat, directed=False, loops=True)\n",
    "\n",
    "\n",
    "def sample_graph(latent):\n",
    "    p = p_from_latent(latent, rescale=False, loops=False)\n",
    "    return sample_edges(p, directed=False, loops=False)\n",
    "\n",
    "\n",
    "def compute_t_stat(sample1, sample2):\n",
    "    test = DCorr()\n",
    "    u, v = k_sample_transform(sample1, sample2, is_y_categorical=False)\n",
    "    return test.test_statistic(u, v)[0]\n",
    "\n",
    "\n",
    "def node_wise_2_sample(latent, node_ind):\n",
    "    node_latent_pop1 = np.squeeze(latent[:n_graphs, node_ind, :])\n",
    "    node_latent_pop2 = np.squeeze(latent[n_graphs:, node_ind, :])\n",
    "    t_stat = compute_t_stat(node_latent_pop1, node_latent_pop2)\n",
    "    return t_stat\n",
    "\n",
    "\n",
    "def compute_pop_t_stats(pop_latent):\n",
    "    n_verts = pop_latent.shape[1]\n",
    "    t_stats = np.zeros(n_verts)\n",
    "    for node_ind in range(n_verts):\n",
    "        t_stat = node_wise_2_sample(pop_latent, node_ind)\n",
    "        t_stats[node_ind] = t_stat\n",
    "    return t_stats\n",
    "\n",
    "\n",
    "def bootstrap_population(latent, n_graphs, seed):\n",
    "    np.random.seed(seed)\n",
    "    bootstrapped_graphs = []\n",
    "    for g in range(n_graphs):\n",
    "        graph = sample_graph(latent)\n",
    "        bootstrapped_graphs.append(graph)\n",
    "\n",
    "    omni = OmnibusEmbed(n_components=2)\n",
    "    bootstrapped_latent = omni.fit_transform(bootstrapped_graphs)\n",
    "    bootstrap_t_stats = compute_pop_t_stats(bootstrapped_latent)\n",
    "    return bootstrap_t_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_p = np.array([[0.25, 0.05], [0.05, 0.15]])\n",
    "n_graphs = 4\n",
    "diff = 1\n",
    "verts_per_block = 64\n",
    "n_verts = 2 * verts_per_block\n",
    "n = 2 * [verts_per_block]\n",
    "node_labels = n_to_labels(n).astype(int)\n",
    "temp = []\n",
    "node1 = []\n",
    "node_list = []\n",
    "\n",
    "# test settings\n",
    "sims = 20\n",
    "n_bootstraps = 50\n",
    "n_jobs = -2\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(range(sims)):\n",
    "    if verbose:\n",
    "        print(f\"Running simulation {x+1}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating graph populations\")\n",
    "    vertex_assignments = np.zeros(n_verts, dtype=int)\n",
    "    vertex_assignments[verts_per_block:] = 1\n",
    "    degree_corrections = np.ones(n_verts)\n",
    "\n",
    "    # Population 1\n",
    "    graphs_pop1 = []\n",
    "    for i in range(n_graphs):\n",
    "        graphs_pop1.append(dcsbm(vertex_assignments, block_p, degree_corrections))\n",
    "    graphs_pop1 = np.array(graphs_pop1)\n",
    "\n",
    "    # Population 2\n",
    "    degree_corrections[0] += diff\n",
    "    degree_corrections[1:verts_per_block] -= diff / (verts_per_block - 1)\n",
    "\n",
    "    graphs_pop2 = []\n",
    "    for i in range(n_graphs):\n",
    "        graphs_pop2.append(dcsbm(node_labels, block_p, degree_corrections))\n",
    "    graphs_pop2 = np.array(graphs_pop2)\n",
    "\n",
    "    n_components = 2\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Doing Omnibus Embedding\")\n",
    "    omni = OmnibusEmbed(n_components=n_components, algorithm=\"randomized\")\n",
    "    graphs = np.concatenate((graphs_pop1, graphs_pop2), axis=0)\n",
    "    pop_latent = omni.fit_transform(graphs)\n",
    "    print(pop_latent.shape)\n",
    "\n",
    "    # Bootstrapping\n",
    "    if verbose:\n",
    "        print(f\"Running {n_bootstraps} bootstraps\")\n",
    "\n",
    "    avg_latent = np.mean(pop_latent, axis=0)\n",
    "\n",
    "    def bsp(seed):\n",
    "        return bootstrap_population(avg_latent, n_graphs * 2, seed)\n",
    "\n",
    "    seeds = np.random.randint(1e8, size=n_bootstraps)\n",
    "    out = Parallel(n_jobs=n_jobs, verbose=verbose)(delayed(bsp)(seed) for seed in seeds)\n",
    "    nulls = np.array(out).T\n",
    "    \n",
    "    \n",
    "    sample_t_stats = compute_pop_t_stats(pop_latent)\n",
    "    node_p_vals = np.zeros(n_verts)\n",
    "    for i, sample_t in enumerate(sample_t_stats):\n",
    "        num_greater = len(np.where(sample_t < nulls[i, :])[0])\n",
    "        p_val = num_greater / n_bootstraps\n",
    "        if p_val < 1 / n_bootstraps:\n",
    "            p_val = 1 / n_bootstraps\n",
    "        node_p_vals[i] = p_val\n",
    "    node1.append(node_p_vals[1])\n",
    "    node_list.append(node_p_vals)\n",
    "#save all nodes\n",
    "print(node_list)\n",
    "\n",
    "#display distribution of p-vals for a single node\n",
    "node1=pd.Series(node1, name=\"p_vals\")\n",
    "sns.distplot(node1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of graph embedding/clustering methods for SBMs\n",
    "Purpose: Want to see which embedding/clustering methods work better on sbms with different settings of # of blocks and B matrix \n",
    "<br>\n",
    "Go through list of n_verts\n",
    "<br>\n",
    "B matrix of k blocks with diagonal values p and other values q\n",
    "<br>\n",
    "Have k constant or increase linearly with n_verts, have q constant or decrease with n_verts\n",
    "<br>\n",
    "Create sbm and embed with either ASE, LSE, or LGC (local graph clustering)\n",
    "<br>\n",
    "Cluster and compute average ARI and standard error of ARI over a number of sims with KMeans or GMM\n",
    "<br> \n",
    "Concluded that k does not affect ARI as much as q\n",
    "<br>\n",
    "Code for diff embed simulation below (LGC not included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "from graspy.simulations import sbm\n",
    "from graspy.embed import AdjacencySpectralEmbed, LaplacianSpectralEmbed\n",
    "from graspy.plot import heatmap\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from src.utils import n_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 methods for k\n",
    "constant_k = 2\n",
    "def linear_k(slope, n):\n",
    "    k = n / slope\n",
    "    k = int(k)\n",
    "    return k\n",
    "\n",
    "#2 methods for q\n",
    "constant_q = 0.5\n",
    "def decay_q(slope, n):\n",
    "    q = slope / n\n",
    "    return q\n",
    "\n",
    "#Generate B Matrix\n",
    "def B_matrix(k, p, q):\n",
    "    B = np.zeros((k,k))\n",
    "    np.fill_diagonal(B, p) \n",
    "    B[B == 0] = q\n",
    "    return B\n",
    "\n",
    "def avg_ari(slope, n_verts, n_sims, p, embed, const_k=True, const_q=True):\n",
    "    temp = []\n",
    "    ari_vals = []\n",
    "    stand_error = []\n",
    "        \n",
    "    #Generate graph\n",
    "    for n in n_verts:\n",
    "        for _ in range(n_sims):\n",
    "            \n",
    "            if const_k:\n",
    "                k, k_func = constant_k\n",
    "            else:\n",
    "                k = linear_k(slope, n)\n",
    "                k_func = f\"n_verts / {slope}\"\n",
    "                \n",
    "            if const_q:\n",
    "                q, q_func = constant_q\n",
    "            else:\n",
    "                q = decay_q(slope, n)\n",
    "                q_func = f\"{slope} / n_verts\"\n",
    "                \n",
    "            B = B_matrix(k, p, q)\n",
    "            cn = [n//k] * k\n",
    "            labels_sbm = n_to_labels(cn).astype(int)\n",
    "            G = sbm(cn, B)\n",
    "\n",
    "            #embedding and clustering\n",
    "            Xhat = embed.fit_transform(G)\n",
    "            \n",
    "            #can be KMeans or GMM\n",
    "            clust = KMeans(n_clusters = k)\n",
    "            labels_clust = clust.fit_predict(Xhat)\n",
    "            ari = adjusted_rand_score(labels_sbm, labels_clust)\n",
    "            temp.append(ari)\n",
    "            print(\"n_verts: {} ARI: {}\".format(n, ari))\n",
    "\n",
    "        ari_vals.append(np.sum(temp) / n_sims)\n",
    "        stand_error.append(stats.sem(temp))\n",
    "        temp = []     \n",
    "    return ari_vals, stand_error, k_func, q_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_verts = [80, 120, 160, 200, 240, 280, 320, 360, 400]\n",
    "slope = n_verts[0] / 2\n",
    "p = 0.5\n",
    "n_sims = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ase = AdjacencySpectralEmbed()\n",
    "embed_lse = LaplacianSpectralEmbed()\n",
    "\n",
    "ari_vals_ase, stand_error_ase, k_func, q_func = avg_ari(slope, n_verts, n_sims, p, embed_ase, const_k=False, const_q=False)\n",
    "ari_vals_lse, stand_error_lse, _, _ = avg_ari(slope, n_verts, n_sims, p, embed_lse, const_k=False, const_q=False)\n",
    "\n",
    "plt.errorbar(n_verts, \n",
    "             ari_vals_ase, \n",
    "             yerr=stand_error_ase,\n",
    "             marker='s',\n",
    "             mfc='red',\n",
    "             label=\"ASE\")\n",
    "plt.errorbar(n_verts, \n",
    "             ari_vals_lse, \n",
    "             yerr=stand_error_lse,\n",
    "             marker='s',\n",
    "             mfc='blue',\n",
    "             label=\"LSE\")\n",
    "plt.title(f\"k = {k_func}, p = {p}, q = {q_func}\") \n",
    "plt.xlabel(\"n_verts\")\n",
    "plt.xticks(n_verts)\n",
    "plt.ylabel(\"ARI\")\n",
    "plt.legend(loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
